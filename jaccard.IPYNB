{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0b61d586c58b45665870d16805ffda3437cb8157ead3cc31938f061d964471e08",
   "display_name": "Python 3.7.9  ('twit': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b61d586c58b45665870d16805ffda3437cb8157ead3cc31938f061d964471e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimus import Optimus\n",
    "from IPython.display import display\n",
    "import tweepy\n",
    "import nest_asyncio\n",
    "import sqlite3\n",
    "import time \n",
    "from pyspark.sql import SparkSession\n",
    "import ipyparallel as ipp\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"jaccard\").getOrCreate()\n",
    "dbname = \"onpoli_graph.db\"\n",
    "conn = sqlite3.connect(dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ****JACARD INDEX FUNCTION****\n",
    "#\n",
    "#   The commented out lines finds union \n",
    "#   directly from the Spark DataFrames. It doesn't work because \n",
    "#   data corruption occurs when loading the SQLite db into Spark\n",
    "#   via JDBC--some ids are negative and unsearchable. It could \n",
    "#   have to do with the fact that when populating the sqlite db \n",
    "#   the 'bio' data was not cleaned (removal of emojis etc.) and \n",
    "#   this caused some unforseen bugs. \n",
    "#\n",
    "#   To compensate there is code that fetches the union data straight \n",
    "#   from the SQLite db. This ISN'T expensive because the union\n",
    "#   of followers between two pillar members are simply the total\n",
    "#   sum of followers between them minus the intersect (which is \n",
    "#   expensive to calculate hence Spark). However the sum is already \n",
    "#   partially stored in the pillars TABLE, granted they\n",
    "#   still need to be added. \n",
    "#\n",
    "#   To further imporve speeds I tried using multiprocessing\n",
    "#   and ipyparallel to call this function but unfortunately I ran \n",
    "#   into too many problems.\n",
    "\n",
    "def jaccard(id_1, id_2, df_mega, f_count):\n",
    "    start = time.time()\n",
    "    intersect = df_mega.filter(df_mega.following == id_1)\\\n",
    "        .select(\"id\")\\\n",
    "        .intersect(df_mega.filter(df_mega.following == id_2)\\\n",
    "        .select(\"id\"))\\\n",
    "        .count()\n",
    "    union = f_count.get(id_1) + f_count.get(id_2) - intersect\n",
    "    print(time.time()-start)\n",
    "    \n",
    "    return intersect/union, intersect\n",
    "\n",
    "    # f_count1 = df_pillars.filter(df_pillars.id == id_1)\\\n",
    "    #     .select(\"followers_count\").collect()[0][0]\n",
    "    # f_count2 = df_pillars.filter(df_pillars.id == id_2)\\\n",
    "    #     .select(\"followers_count\").collect()[0][0]\n",
    "    # union = f_count1 + f_count2 - intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Loads Dataframe into spark\n",
    "\n",
    "df_mega = spark.read.format('jdbc') \\\n",
    "        .options(driver='org.sqlite.JDBC',\n",
    "                 dbtable='mega_ids',\n",
    "                 url=\"jdbc:sqlite:C:/Users/Daniel/ProjTwit/onpoli_graph.db\",\n",
    "                 numPartitions = 1,\n",
    "                 )\\\n",
    "        .load()\n",
    "\n",
    "#   Don't need this df, but it's nice to have if you want to see the data \n",
    "#   Also usable if the dataframe's data wasn't corrupt\n",
    "\n",
    "# df_pillars = spark.read.format('jdbc') \\\n",
    "#         .options(driver='org.sqlite.JDBC', dbtable='pillars',\n",
    "#                  url=\"jdbc:sqlite:C:/Users/Daniel/ProjTwit/onpoli_graph.db\",\n",
    "#                  numPartitions = 1)\\\n",
    "#         .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.7074451446533203\n1---14216661:14260108 took 3.7084436416625977\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-753b8def3163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mnode2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mdf_mega\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mfollowers_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             )\n\u001b[0;32m     30\u001b[0m             \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"INSERT OR IGNORE INTO jaccard_edge VALUES(?,?,?)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "with conn:\n",
    "#   Code fetches union data directly from SQLite table\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT id FROM pillars\"\n",
    "    cursor.execute(query)\n",
    "    keys = [x[0] for x in cursor.fetchall()]\n",
    "    query = \"SELECT followers_count FROM pillars\"\n",
    "    cursor.execute(query)\n",
    "    vals = [x[0] for x in cursor.fetchall()]\n",
    "    followers_count = dict(zip(keys, vals))\n",
    "\n",
    "#   This is the only way to sequentially calculate Jaccard\n",
    "#   indexes between each and every node. \n",
    "#   It runs in (O(n)^2-O(n))/2 time--which is clear\n",
    "#   by the nature of the forloops\n",
    "    count=0\n",
    "    start_big = time.time()\n",
    "    for i in range(len(keys)):\n",
    "        node = keys[i]\n",
    "        for k in range(i+1, len(keys)):\n",
    "            count += 1\n",
    "            node2 = keys[k]\n",
    "            start = time.time()\n",
    "            jaccard_i, intersect = jaccard(\n",
    "                node,\n",
    "                node2, \n",
    "                df_mega, \n",
    "                followers_count\n",
    "            )\n",
    "            query = \"INSERT OR IGNORE INTO jaccard_edge VALUES(?,?,?)\"\n",
    "            entry = (node, node2, jaccard_i)\n",
    "            cursor.execute(query, entry)\n",
    "            conn.commit()\n",
    "            query = \"INSERT OR IGNORE INTO follower_intersect VALUES(?,?,?)\"\n",
    "            entry = (node, node2, intersect)\n",
    "            cursor.execute(query, entry)\n",
    "            conn.commit()\n",
    "            duration = time.time()- start\n",
    "            print(f\"{count}---{node}:{node2} took {duration}\")\n",
    "    \n",
    "    dur = time.time() - start_big\n",
    "    print(f\"total time {dur}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}